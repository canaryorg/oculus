\chapter{System design}

The system, from here to be refered to by the name ,,\textit{Oculus}'', is designed with an asynchronous as well as distributed aproach in mind. In order to achieve high asynchronousity between obtaining new reference data, and running jobs such as ,,compare video1 with the reference database'', the system was split into two primary components: 

\begin{itemize}
  \item \textbf{downloader} -- which is responsible for obtaining more and more reference material. It persists and initially processes the videos, as well as any related metadata,
  \item and the \textbf{job runner} -- which is responsible for running computations on top of the hadoop cluster and reference databases.
\end{itemize}

In this chapter I will discuss the high-level design of these subsystems, and how they interact with each other. In the next chapter I will describe each of the systems technical challanges and how they were solved.

\section{Downloader}
The downloader is responsible for obtaining as much as possible ,,reference data'', by which I mean video material -- from sites such as \textit{youtube.com} or video hosting sites.

It should be also noted that while I refer to this system as ,,the'' downloader, in fact it is composed out of many instances of the same application, sharing the common workload of downloading and converting the input data. The deployment diagram on drawing [12] explains the interactions between the nodes.

\subsection{Work sharing in an actor-based cluster}
The downloader system is implemented in an ,,message pasing'' style, which means that the work requests to the cluster come in as \textit{messages}, and then are \textit{routed} to an \textit{actor}, who performs the work, and then responds with another message -- in other words, all comunication between Actor instances is performed via messages and there is no shared state between them. 
This also applies to Actor instances residing in the same JVM - for the user of an ,,Actor System'', the location (on which phisical node the actor is actually executing) of an actor remains fully transparent - this has huge gains in terms of load balancing the message execution on the cluster - the router decides who will be notified on which message, the listing \ref{akka-router-decides} shows a typical workflow using the so-called ,,smallest inbox'' routing strategy \ref{akkaDocs}.

% TODO replace with a drawing with actors
\begin{verbatim*}
                                                                    ---> [inbox1, size = 3] => YoutubeDownloadActor(1)
 YoutubeCrawlerActor -- please download [http://...] --> router ---/
                                                                   \     [inbox2, size = 5] => YoutubeDownloadActor(2)
                                                                   
                                            /* because inbox1.size < inbox2.size */ 
\end{verbatim}

The underlying Actor System is implemented by a project called Akka \ref{akkaDocs}, and can be easiest explained as ,,porting Erlang concepts of the Actor Model to the JVM''.

As mentioned before, the system is fully distributed and \textit{any node can perform any task} submited to the cluster. For example let's take the first step in the processing pipeline in Oculus, which is \verb|Download video from http://www.youtube.com/watch?v=-X9bcrJ3TjY| -- such message will be emited by the \verb|YoutubeCrawlerActor| and sent via a \verb|Router| instance to an \verb|YoutubeDownloadActor| which has the ,,smallest inbox''.


\section{Job Runner}
The job runners responsility is at the core of the systems