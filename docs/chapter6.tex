\chapter{Conclusions}
\label{chap:conclusions}

The primary goal of this thesis was to research and scale distributed media processing systems. This has been achieved by implementing a reference system, which replicated the needs and workload of a quite realistic system that would have to analyse video data. This system was then used as testbed for scaling Hadoop and Akka clusters.

The implemented reference system was able to fulfil it's initial requirements of detecting ``near duplicate'' movies.
However the secondary requirement of finding a scenes inside of movies has been quite hard to implement and still required manual evaluation of the Map Reduce jobs' results. Several test runs using publicly available creative-commons licensed movies have been performed. The system was also tuned to perform in the face of maliciously modified video content (mirrored, blurred or otherwise distorted), and the system was still able to identify matching video data. Certain optimisations were applied to make the search times possibly fast. These optimisations required gaining knowledge about Hadoop's and HBase's internals and designing a specialised row key, as has described in Chapter \ref{chap:analysis-examples}. On the perceptual hashing front there is still much work that could have been applied to the proposed system, and better fingerprinting methods could have been applied -- yes as this was not the primary focus of this thesis, the proposed system performed within it's expected expected performance boundaries of a few hours of processing per each movie.

The reference system was then used to perform multiple scalability tests and optimisations, as described in Chapter \ref{chap:perf-scalability}. The Hadoop cluster was scaled out horizontally by adding mode nodes and measuring it's performance improvement -- which turned out to be nearly linear, if one trades of storage duplication for computation speed, or slightly lower if replication factors would not be tuned in accordance to the clusters size. The cluster had also undergone configuration as well as application level improvements aimed at utilising HDFS at it's best.

To conclude, the investigated distributed systems provide an astounding platform to build media analysis applications on. Akka as well as Hadoop and it's rich eco-system allow to easily build resilient and scalable applications which are able to process huge amounts of data. Hadoop especially confirmed it's ability to scale-out very easily, and has proven to be a very stable execution platform. The operational overhead of starting clusters that such applications require can be lessened by far thanks to using modern configuration provisioning utilities such as Chef, and public cloud providers make testing clusters in varying sizes very easy. In the foreseeable future we will see more kinds of these tools being perfected, as the demand for applications of such scale is still growing.