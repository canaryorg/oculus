% ------------------------------------------------------------------------------------------------------------------------------------------------
\chapter{Analysis Job implementation examples}
\label{chap:analysis-examples}

This chapter aims to provide tangible examples of how the previously described system works and in what way the parallel nature of the implemented system benefited these use cases.

The following two sections will focus on practical examples of so-called ''attacks'' (distortions) applied to the input media data, and how the proposed system has handled it. The examples have been selected to highlight the two major problems the system has to handle -- distorted image data and time shifted data.

In Section \ref{sec:mirrored-video-detection} video material will be analysed in order to find it's corresponding ''mirrored'' counter-part. This example will also be used to highlight the tremendous possibilities that lie within data \textit{pre-processing} that are applied within the proposed system, and if needed could be expanded even more in order to speed up the system's initial response time.

In Section \ref{sec:scene-detection} an extracted scene will be positioned within an existing video in the reference database. This problem turns out to be non-trivial because of different frame-rates of supplied material, thus search methods similar, in concept, to sub-string search could not have been applied efficiently. The section explains the algorithms applied instead, and touches upon the problem of interpreting results of complex map reduce jobs.

Section \ref{sec:summing-up-analysis} summarises how the approach to designing pipelines and data representation used in this chapter was influenced by the the large amounts of ''fuzzy'' data, and how these algorithms could be further expanded.

\textit{As this thesis is not focused on development of image analysis algorithms, these sections will instead lean their focus towards the distributed system and big-data parts of the problem. The image comparison algorithms also assume that that the used image correlation algorithm (\textit{phash} -- see Section \ref{sec:phash}) performs well enough for the job at hand. }

% -----------------------------------------------------------------------------------
\section{Near-duplicate video detection}
\label{sec:mirrored-video-detection}
In order to test the system in scenarios of image distortion the example case of ''mirrored'' video material was first used. This case is fairly popular among material uploaded to YouTube, as content uploaders often ''flip'' the uploaded content in order to avoid copyright detection to trigger on content.

As will be shown in the example section, this ''attack'' is not very effective, as determining a exact-mirror video material is fairly simple. Even in the case of slight hue and luminescence changes the implemented system was able to detect such videos flawlessly.

\subsection{Analysed example material}
\textit{The video material used in this chapter to exemplify the described attack scenarios originates from the movie ''\textit{Big Buck Bunny}'' \cite{big-buck-bunny} which has been created by the Blender Foundation \cite{blender-foundation} and released under the Creative Commons license \cite{creative-commons}.}

Figure \ref{fig:frames-mirrored} for example, illustrates the original as well as mirrored counter part of an example frame taken from the movie. The movie is 9 minutes and 59 seconds long. The version used during this analysis is the original 1080p resolution version, recorded by the Blender foundation. The entire movie was stored in the reference database in raw format, which amounts amounts to a total size of 6.25 GB.


\begin{figure}[ch!]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{img/Big_Buck_Bunny_normal.png}
                \caption{Original frame}
                \label{fig:original-frame}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{img/Big_Buck_Bunny_mirror.png}
                \caption{Mirrored frame}
                \label{fig:mirrored-frame}
        \end{subfigure}
        \caption{Example of original and mirrored frame}
        \label{fig:frames-mirrored}
\end{figure}





\subsection{Histogram calculation and HBase row key design}
The Map Reduce pipeline designed for detecting near-duplicate movies takes one very important fact into account: the histogram of an nearly identical movie will most likely not change very much. In the case of merely mirrored video material, the histogram will not change at all. This fact was utilised to design the row key for storing histograms in HBase for efficient lookup.

\begin{figure}[ch!]
  \centering
  \includegraphics[width=\textwidth]{img/Histograms__Big_Buck_Bunny_mirror_png.png}
  \caption{R/G/B Histograms of analysed frame}
\end{figure}

Hbase's implementation provides strict guarantees about an HTables row key, specifically it guarantees to store data in \textit{alphabetically sorted order} based on it's row key. This property \textit{must} be taken into account when designing tables and keys in HBase. In particular it is a widely used technique to store parts of the data, or projections of it, directly in the key itself -- this is because HBase relies so much on the key to distribute data among RegionServers and a wrongly designed key may easily hot-spot one of the region servers (even in the presence of so called region-splitting, which re-shards the more work-intense regions onto multiple servers).

In our example, after obtaining the histogram data for each of the frames of an analysed movie, it is stored in the \verb|Histograms| table, with a precisely defined key. The key is designed to allow \textit{prefix queries} which are incredibly fast -- because HBase is able to determine, just by looking at the prefix query range, which exact RegionServers must be contacted, and they in turn are able to perform a linear scan of just the required region of the Table (which often means reading from disk). The key was designed using these guiding principles:

\begin{itemize}
 \item the key must allow for prefix queries, which given a ''to be compared frame'' will return ''probable candidates''
 \item more precise queries must also be possible to be constructed as prefix queries
\end{itemize}

Having this in mind, the key is designed to be an descending sorted \verb|-| joined list of colour dominance in a given frame. Colour dominance is calculated based on the percent of pixels in the image where one colour's part would dominate (have a higher value) than any other colour parts. For example given a pixel  \verb|RGB("#F2649D") = RGB(100, 242, 157)|, this pixel would be deemed to be \textit{green dominating}. 

\begin{lstlisting}[caption={Grammar for the Histogram RowKey. In essence, it contains an encoded colour dominance value, the youtube id of the movie, and frame number.}, label={lst:row-key}]
    NUMBER           : [0-9]                              ;
    YT_ID            : .+                                 ; // youtube id
    FRAME            : NUMBER+                            ; // frame number
    COLOR            : ("R" | "G" | "B") NUMBER           ; // dominance of color in frame
    HIST_ORDERED : COLOR "-" COLOR "-" COLOR        ; // sorted by dominance

    ROW_KEY          : HIST_ORDERED ":" YT_ID ":" FRAME ;
    
    // examples: R57-B28-G13:YE7VzlLtp-4.mp4:22009  <- Red dominates
    //               G47-R41-G11:YE7VzlLtp-4.m6p4:919    <- Green dominates
\end{lstlisting}

Listing \ref{lst:row-key} provides a simple ANTLR \footnote{ANTLR is a popular parser generator library for Java applications} inspired grammar, which should make the key's parts easily visible (while ANTLR was not used directly for parsing of the key, it is a very clear and concise way of explaining the key's structure).

Having defined such key, Map Reduce jobs populate the reference database using it. Of course, the entire histograms would still be stored within the row (all 255 values for each color). The rest of the HTable's design is depicted on Figure \ref{fig:histogram-table}.

\begin{figure}[ch!]
  \centering
  \includegraphics[width=\textwidth]{img/hbase-hashes-table}
  \caption{HBase HTable design used for histogram based lookups.}
  \label{fig:histogram-table}
\end{figure}

This design allows the Analyser to issue very efficient queries for frames, such as in the examples shown in Table \ref{tab:scans}. For one matching job, the analyser will issue multiple HBase queries, broadening the scope of the search if no full match has been found in the first round, avoiding having to issue slower Scan operations to HBase.

\begin{table}[ch!]
  \centering
  \begin{tabular}{|c|c|c|}
  \hline
  \textbf{Question}                  & \textbf{Query}                    & Prefix Scan \\ \hline
  Blue dominated frames              & \verb|START "B", END "B99"|       & Yes; Fast! \\ \hline
  Strongly red dominated frames      & \verb|START "R9", END "R99"|      & Yes; Fast! \\ \hline
  Green and Blue dominated frames (grass + sky) & \verb|FILTER "G*B*"|   & No; Slow \\ \hline
  All frames of Movie                & \verb|FILTER "*-e98uKex-*"|       & No; Slow \\ \hline
  \end{tabular}
  \caption{Examples of queries enabled by such RowKey design}
  \label{tab:scans}
\end{table}


\subsection{Role of perceptual hashing in near-duplicate detection}
\label{sec:perceptual-hashing}
Since simple histogram comparison if obviously not enough to determine if an image is of the same movie or not,
the second layer of comparison employed by the Analyser's Map Reduce Jobs is powered by perceptual hashing, as implemented by the phash \cite{phash} library.

As can be seen in Figure \ref{fig:histogram-table}, rows in the Histograms table contain not only the histogram values, but also the phash column family as well as youtube metadata. In this step of execution we will be comparing the already pre-filtered (by using the previous method) frames by their perceptual hashes. A detailed description and effectiveness analysis of these is available on the libraries website \cite{phash}.

During this work two perceptual hash implementations were used:
\begin{itemize}
  \item DCT hash -- Discrete Cosine Transform Hash; Is an efficient means to calculate an image hash based on it's
                    frequency spectrum data. It is usually not sufficient in order to determine image similarity, 
                    but is a good accompanying hash function, which can be used in pair with other functions to increase 
                    their accuracy. If DCT does not match the given images, then most probably other algorithms also 
                    should not. It is also fairly efficient in detecting slight rotation and blurring. Experimentation 
                    conducted during this thesis show that a distance value lower than 20 points usually indicates a 
                    ''possible match'' using this algorithm.
                    
  \item MH hash -- based on the Marr--Hildreth algorithm \cite{marr-hildreth}, which is an edge detection algorithm.
                   The resulting ''hash value'' is in fact an compressed representation of the edges detected within the 
                   analysed image. This hash has a constant length of 72 bytes long hash, and can be compared efficiently 
                   by using the classical Hamming Distance \cite{hamming-distance} method. This hashing algorithm is
                   usually sufficient to determing image equality. It is not very resilient against image rotation, but 
                   manages very well with blurring and differences in compared image resolutions. Used in pair with the 
                   DCT hash and histogram methods it is a very viable approach to near-equality checks.
\end{itemize}

Listing \ref{lst:phashes-example} presets example hashes calculated for the frames seen in Figure \ref{fig:frames-mirrored}. It's worth pointing out that the dct hash in both cases is equal, while the mh hash is different changed - because it carries more information in it's 72 bytes.

\begin{lstlisting}[caption={Example hashes, calculated for a original and mirrored frame}, label={lst:phashes-example}]
// Big_Buck_Bunny_normal.png
dctHash = 54086765383280
mhHash  = 8e2f6d04831568425b3c5c2ca01afd88b6ad638d43ced55d71cc032c53a
            bdc898e930523e00fb334e765e57529f0ca5b6b6333a35076b1fd249a1e
            00e0e7c45beaa10792bc453228
                        
// Big_Buck_Bunny_mirror.png
dctHash = 54086765383280
mhHash  = 458193aa7ca2e2f03e29065f29f01019f4b813f07b217f94775dcb91985
            cc0ef83656b981b575f4d44ad7848c5469d1ad81a496665c3e262070e42
            4d8592231a1c8118fcf2f0ef63
\end{lstlisting}

Hashes like this can be compared to each other using a simple Hamming Distance \cite{hamming-distance} function, as their values at certain positions represent certain traits of the hashed picture. This is the core of the last phase of the matching algorithm. The Table \ref{tab:hash-distances} shows how the distance between these hashes differ in a few example scenarios. As is to be expected, the mh hash is reacting much more than the plain DCT hash. 

\begin{table}[ch!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline 
    \textbf{Tested distortion}             & \textbf{DCT distance} & \textbf{MH distance} \\ \hline
    Mirrored frame                         & 0                     &  402 \\ \hline
    +20\% luminosity                       & 0                     &  132 \\ \hline
    Blurring                               & 0                     &  246 \\ \hline
    Slightly different frames (next frame) & 25                    &  324 \\ \hline
    Different frames (moved character)     & 31                    &  355 \\ \hline
    
  \end{tabular}
  \caption{Example distances between sample frames, and their distorted versions.}
  \label{tab:hash-distances}
\end{table}

After a certain number of experimental runs with the system, it was empirically decided that images whose distance values ranged below 40 for DCT-hash \textit{and} very near or below 350 for the MH-hash can be \textit{safely} considered ''almost identical''.

Results from such a movie comparing job pipeline are yielded onto HDFS, in form of a simple CSV file, which is sorted by multiple fields: first the input frame, then the histogram distance and then the hash distances. Using this method it's possible to determine ''top N'' most probable matches for a given frame. Listing \ref{lst:mirrored-results} shows good top matches -- such the dct distance is below 40 and the mh distance below 370. In this case, since the analysed movie was a mirror the histogram difference (calculated in a very simplistic way -- as hamming distance of the row-key encoded histogram data) is equal 0 for identical (just mirrored) frames, and is slightly off for cases where the reference database did not contain the exact now matched against frame. Since the reference data is not stored for every frame for movies, but for 10 frames per second (instead of the usual 25 in PAL or 30 in NTSC regions), the granularity of the matches will be down sampled to a granularity of 10 frames. This lower data granularity is the reason that some frames will match a very near frame in the reference database. This storage-space optimisation could be discarded if high quality matches would be required by the system.

\begin{lstlisting}[caption={Examples of valid ''best'' matches for a few sample frames (mirrored movie).}, label={lst:mirrored-results}]
...
F   7809 => dist(dct: 38, hist: 3, mh: 376) => YE7VzlLtp-4 @  7789   (20 off)
F 14409 => dist(dct: 34, hist: 0, mh: 349) => YE7VzlLtp-4 @ 14409 ( 0 off)
F   2599 => dist(dct: 37, hist: 2, mh: 351) => YE7VzlLtp-4 @  2569   (30 off)
F 54919 => dist(dct: 32, hist: 0, mh: 307) => YE7VzlLtp-4 @ 54919 (0 off)
...
\end{lstlisting}

Listing \ref{lst:mirrored-results-wrong} on the other hand, shows example values for an invalid matches. All distances are unreasonably high -- especially such high value of the dct hash distance disqualifies these frame matches from further processing.

\begin{lstlisting}[caption={Examples of invalid matches for a few sample frames.}, label={lst:mirrored-results-wrong}]
...
F 4299 => dist(dct: 53, hist: 3, mh: 395) => YE7VzlLtp-4 @ 57809
F 7539 => dist(dct: 47, hist: 2, mh: 384) => YE7VzlLtp-4 @ 29939
...
\end{lstlisting}


\subsubsection{Movie matching pipeline summary}
To sum up how the pipeline was designed, and how the previously described parts fit into the bigger picture Figure \ref{fig:full-job} explains how the different operations are connected into the pipeline.

\begin{figure}[ch!]
  \centering
  \includegraphics[width=0.55\textwidth]{img/top-match-job}
  \caption{Map Reduce Pipeline, designed for matching ''most similar'' movie in reference database, when an attacked video file is given.}
  \label{fig:full-job}
\end{figure}

Describing the Pipeline depicted on Figure \ref{fig:full-job} in further detail: First the attacked movie material must be hashed as well as it's histograms must be calculated (1). This data is then persisted into HBase. The Join operation (3) forces the HBase read (2) and performs the joining operation. The now merged data stream is then used to calculate the hash and histogram distances between all (millions) joined frames (4). Next, the data is grouped by the compared movie's frame numbers -- this allows us to find the best match for each frame, by simply sorting the sequence of data grouped for each of the frames (5). Scalding allows to perform this operation quite optimally -- so that not the entire stream must be sorted, we simply find the best value and stop sorting. Next the data is again grouped, but this time against the matched movie ids (6), which allows us to count how many frames matched to a given movie. Lastly, simply sorting the output of this Job by the number of matched frames (7) allows to find a movie that "the most frames have been matched to".


% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scene positioning}
\label{sec:scene-detection}
This scenario can be explained as trying to find out \textit{where} (if at all) a scene takes place in a movie known to the reference database. 

\begin{figure}[ch!]
  \centering
  \includegraphics[width=0.55\textwidth]{img/frames-timeline-matching}
  \caption{Visual representation of the goal of this example application.}
\end{figure}

Although on the surface the general problem statement is not so different than substring search, which is a known and well researched topic in computer science. In sub-string search algorithms like the Knuth–Morris–Pratt \cite{kmp-string-search} or the Boyer-Moore \cite{boyer-string-search} algorithms leverage that the ''matching'' either will apply, or will not in order to increase search speed in the worst case to still linear time. However, these methods can \textit{not} be directly applied to the problem specified in this section -- because of the distortions in source and reference video material.

Additionally the possibility of cross-matches when a movie is built up from multiple short scenes from other movies. The most popular example would be be ''flash-back'' scenes or ''top 10'' movies where before the last top-3, the movie would quickly go over already shown frames of scenes. Another problem adding to the distortions is frame rates of reference data vs. an analysed video fragment -- even a slight mismatch (30FPS vs. 25FPS) would render the substring search algorithms not usable for this concrete example.

\begin{figure}[ch!]
  \centering
  \includegraphics[width=0.45\textwidth]{img/frames-timeline-matching-missmatch}
  \caption{A frame may match multiple reference movies, complicating the task significantly.}
\end{figure}



Instead, a more statistical approach was taken. The basic idea of this Pipeline can be described as trying to match frames between 2 movies in similar style as in the previously described pipeline (Figure \ref{fig:full-job}, yet afterwards the top match for each frame must proceed incrementally in the matched frames. For example if scene frame 100 matched reference frame 90100 and scene-frame 110 matched frame 90110 one should expect that this interval will be constant over the entire matched range of the frames. This method can also detect ''top10'' movies, where a movie contains multiple scenes from different movies -- yet this algorithm was deemed to be outside of the scope of this thesis.


\begin{figure}[ch!]
  \centering
  \includegraphics[width=0.6\textwidth]{img/top-match-job-positioning}
  \caption{Pipeline used for Scene positioning within another movie; The last step is performed by an external application analysing the Hadoop jobs' results.}
  \label{fig:job-scene}
\end{figure}

The pipeline designed for solving this problem is shown on Figure \ref{fig:job-scene}. As can be easily observed, the steps 1 through 5 are the same (with the exception of the input from HBase in step (2) being filtered only for the targeted movie). The code used to produce these pipelines can be re-used thanks to Scala and Scalding. The 6th step though needs to be implemented a-new as it requires a new operation: sorting the matched frames in ascending order by the frame number of the analysed movie. This allows us to run an analysis tool (implemented in Scala) that will analyse the result for longest \textit{matching sequence of frames}. The last step (7) can be described as finding such series of frames, that all match to the same movie, and their frame numbers all increase in the same fashion as the frame numbers of the compared scene. This step was not implemented in Hadoop, and turned out to be quite challenging to be implemented resilient enough to provide easy human readable read outputs. 


The problems encountered during the implementation of this pipeline highlight an interesting problem with these pipelines -- extracting the actual information that is present in the computed results is often not trivial, and the noise carried with the data often complicates the tasks significantly.






\section{Impact of big data to the design of the pipelines}
\label{sec:summing-up-analysis}

The approach taken during designing the job pipelines utilises the fact of having humongous amounts of data available to match against. This also produces the problem of possible mis-matches. For example, it is completely expected and normal that some frames may match to the wrong movie. An extreme case of this being ''entirely black'' or ''entirely white'' frames, which appear in the vast majority of movies when fading in or out scenes, or credits. Much of the time spent on designing these pipelines was spent on trying to minimise the amount of mis-matches as well numbers of map reduce passes.

Satisfiable results can be achieved of reasonable amounts of processing time (a few hours per movie), and Hadoop's scaling abilities help to increase computation time when tasks are able to execute concurrently. 

While this section focused on the mirrored movie case, the approach developed during the system's design scales nicely to other distortions too. Other kinds of distortions could be applied to a video, such as comparing a high-quality video with a low-quality counterpart or simple colouring filters. In fact, the tested algorithms used for hash calculation work more efficiently with blurred images -- thus the distance calculation in these cases may be even more precise than in the analysed example.

Summing up, the way to design algorithms when working with big data seems to shift ones' focus towards noise elimination as well as assuring the algorithms can be executed in parallel. Similar principles as with designing parallel algorithms apply to working with big data tasks -- even theoretically expensive jobs, if parallelised, can be executed still in a timely manner.















