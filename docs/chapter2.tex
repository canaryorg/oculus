\chapter{Analysis of available technologies}
\label{cha:existing-and-selected-tech}

As the core of this work will focus on analysing and benchmarking usage of popular distributed system stacks, it is only fair to begin with introducing the selected components from which the system consist.

This chapter should be treated as a brief introduction into the selected technologies, as very detailed explanations and and implementation details will be provided throughout chapters \ref{chap:system-design} through \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache Hadoop}
\label{sec:hadoop}

Apache Hadoop is a suite of tools and libraries modeled after a number of Google's most fameous whitepapers concerning ''Big Data'', such as \textit{Chubby} \cite{chubby} (on which the \textit{Google Distributed File System} \cite{gfs} was built) and later papers like the ground breaking \textit{Map Reduce} \cite{map-reduce} whitepaper concerning parallelisation of computation over massive amounts of data.
The re-implementation of these whitepapers which has become known as Hadoop was originaly an implementation used by Yahoo \cite{yahoo} internally, and then released to the general public in late 2007 under the Apache Free Software License.

The general use-case of Hadoop based system revolves around massively parallel computation over humongous amounts of data. Thanks to employing functional programming paradigms in multi-server environments Hadoop makes it possible, and simple, to distribute so called ''Map Reduce Jobs'' across thousands of servers which execute the given \textit{map} (also known as ''\textit{transform}'') and \textit{reduce} (also known as ''\textit{fold}'') functions in a paralle, distributed fashion. Complex computations, which can not be represented as single Map Reduce jobs, are often executed as a series of jobs, so called Job Pipelines. This method will be leveraged and explained in detail in Chapter \ref{chap:implementation}, together with the introduction of Scalding (see Section \ref{sec:scalding-info}) a Domain Specific Language built specifically to ease building such pipelines.

The promise of Hadoop is practically linear scalability of Hadoop clusters when adding more resources to such cluster -- these claims will be investigates in Chapter \ref{chap:perf-scalability}, where results of different cluster configurations will be compared. The computation model proposed by Hadoop will be examined and explained in detail in later sections of this paper, as it is the dominating model chosen for the implementation of the presented system.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scalding \& Cascading}
\label{sec:scalding-info}

Scalding \cite{scalding} is a Domain Specific Language implemented using the Scala \cite{scala} programming languagg. It has been developer at Twitter \cite{twitter} for their internal needs, and then released under the GPL license. It is aimed at proving a more expressive and powerful language for writing Map Reduce Job definitions, which otherwise
would be implemented in the Java \cite{java}, which would often result in very verbose and hard to understand code (especialy due to the verbosity of Hadoop's core APIs).

Scalding does not stand on it's own, as it is only a thin layer built on top of Concurrent Inc.'s \cite{concurrent-inc} Cascading library, which is a framework built on top Apache Hadoop and enables map reduce authors to think in terms of high level abstractions, such as data ''flows'' 
and job ''pipelines'' (series of Map Reduce jobs executed in parallel or sequentially) which have been used extensively in this project.



% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache HBase}
\label{sec:hbase}

HBase is a column-oriented database \cite{columnar-database} designed by following the Google white paper on their ''BigTable'' datastore published in 2007.
Column oriented storage of data, as opposed to row oriented (as most SQL databases), as huge advantages when many aggregations over only given columns are performed.

It was selected for this project because it's excellent random-access to data as well as being perfectly suited for sourcing Map Reduce tasks. HBase stores it Tables on the Hadoop Distributed File System, thus it scales similarly to it -- because it's Tables are layed out as Sequence Files that are a very performant way to store data (such as rows of a big table) on Hadoop's File System. Detailed investigation in Hadoop as well as Sequence File performance will be provided in a later section in Chapter \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scala}
\label{sec:scala}
Scala is a functional \textit{and} object-oriented programming language designed by Martin Odersky \cite{scala} running on the Java Virtual Machine. It was selected selected as primary implementation language for this project (although other languages used include: ANSI C, Ruby and Bash) because of the compelling libraries for building distributed systems using it, such as \textit{Akka} and \textit{Scalding} (both introduced in this section).

It's functional nature (making it similar to languages such as Lisp or Haskell) is very helpful when performing transform / aggregate operations over collections of data. It should be also noted that Hadoop \textit{itself} was inspired by languages such as this, because the canonical names of the functions performing data transformation and aggregation in functional languages are: ''map'' and ''reduce''.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Akka}
\label{sec:akka}

Akka is a library providing an Actor Model \cite{actor-model} based concurrency for Scala (and Java) applications. 
Using this concurrency model can be best explained using two rules ''share nothing'' and ''communicate via messages'' -- both steming from the programming language Erlang \cite{erlang}, and indeed both Erlang and Akka provide the same concepts and levels of abstraction. The general gain from using this model is that one is oblivious to \textit{where} an Actor (executor of code / reciver of messages) actually is running -- in terms of ''in local JVM'' or ''remotely, and the message will be delivered via TCP/UDP''. Thanks to this not-knowing, it is trivial to scale such applications horizontally, since the code does not need to change when moving from 1-node implementations to clustered environments.

For the sake of this thesis, Akka has been used both in local (in-jvm) parallel execution as well as clustered deployment (using Akka's built in clustering module), in order to balance the workload generated by actors across the entire cluster. Details on scaling Akka clusters have been provided in Section \ref{sec:scaling-akka}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{phash}
\label{sec:phash}
PHash is short for ,,Perceptual Hash'' and is a sort of hashing algorithm (primarily aimed for use with images), which retains enough information to be comparable with another has, yielding ,,how similar'' these hashes are. The details and implementation of it have been explained by Christoph Zauner's \cite{phash}.

This algorithm is used by the system to perform initial similarity analysis between images. The algorithm is publicly available, including sources (in C), and may be used in \textit{non-commercial applications}. During the work on this thesis the source code of phash was slightly modified (in agreement with the software's license) to be adjusted to work better by being called from Java applications.

As the goal of this thesis is not researching such algorithms, but focusing on scalable image analysis computations in distributed systems, it was deeped apropriate to use an existing perceptual hashing solution.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Chef}
\label{sec:chef}
OpsCode Chef is a set of tools aimed at automating server configuration and provisioning. Using it enabeled automating spinning up new servers which was a very large part of the work on this thesis, and also one such automation was prepared, it could also be applied to different cloud service provides -- which lead to exploring local virtual machines (Vagrant and VirtualBox) as well as Amazon's EC2 public cloud offering until lastly satteling for good on using Google's Compute Engine public cloud offering. The use of Chef enabled not spending tedious hour of reinstalling the cluster each time anew, but allowed to ''cook'' given virtual machines into the required state (that is, installing Hadoop, Hbase, Java and all the other dependencies needed for running the system implemented during this work).

Appendix \ref{app:chef} features a detailed overview and guide on the details on how Chef was used in this thesis, and what steps had to be taken in order to prepare the ''recipes'' it works on to be able to provision Hadoop automatically to any given GNU/Linux running instance.


% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Other tools and technologies used}
\label{sec:other tools}

\subsection{youtube-dl}
Youtube-dl is a small library written in python and freely available under an Open Source license. 
It was used in order to make downloading source video files from youtube more efficient, as it is aware of multiple available video formats (high / low quality), and offers multiple options useful yet hard to implement for this project -- including for example ,,preferring to download open source video formats'', which allowed me to avoid installing proprietary video codecs on the servers.

%\subsection{tesseract-ocr}
%
%Tesseract \cite{tesseract} is a text recognition library developed by Google and freely available to use (including language stems for most popular languages).
%This tool has been used in order to extract text from analysed images, providing even more data.
%
% remove sesseract IMO...