\chapter{Analysis of available technologies}
As the core of this work will focus on analysing and benchmarking usage of popular distributed system stacks, it is only fair to begin with introducing the selected components from which the system consist.

This chapter should be treated as a brief introduction into the selected technologies, as very detailed explanations and and implementation details will be provided throughout chapters \ref{chap:system-design} through \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache Hadoop}
\label{sec:hadoop}

Apache Hadoop is a suite of tools and libraries modeled after a number of Google's most fameous whitepapers concerning ,,Big Data'', such as \textit{Chubby} \cite{chubby} (on which the \textit{Google Distributed File System} \cite{gfs} was built) and later papers like the ground breaking \textit{Map Reduce} \cite{map-reduce} whitepaper concerning parallelisation of computation over massive amounts of data.
The re-implementation of these whitepapers which has become known as Hadoop was originaly an implementation used by Yahoo \cite{yahoo} internally, and then released to the general public in late 2007 under the Apache Free Software License.

The general use-case of Hadoop based system revolves around massively parallel computation over humongous amounts of data. Thanks to employing functional programming paradigms in multi-server environments Hadoop makes it possible, and simple, to distribute so called ,,Map Reduce Jobs'' across thousands of servers which execute the given \textit{map} (also known as ,,\textit{transform}'') and \textit{reduce} (also known as ,,\textit{fold}'') functions in a paralle, distributed fashion. Complex computations, which can not be represented as single Map Reduce jobs, are often executed as a series of jobs, so called Job Pipelines. This method will be leveraged and explained in detail in Chapter \ref{chap:implementation}, together with the introduction of Scalding (see Section \ref{sec:scalding-info}) a Domain Specific Language built specifically to ease building such pipelines.

The promise of Hadoop is practically linear scalability of Hadoop clusters when adding more resources to such cluster -- these claims will be investigates in Chapter \ref{chap:perf-scalability}, where results of different cluster configurations will be compared. The computation model proposed by Hadoop will be examined and explained in detail in later sections of this paper, as it is the dominating model chosen for the implementation of the presented system.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scalding \& Cascading}
\label{sec:scalding-info}

Scalding is a Domain Specific Language implemented using the Scala \cite{scala} programming language developer at Twitter \cite{twitter} for their internal needs, and then released under the GPL license. It is aimed at proving a more expressive and powerful language for writing Map Reduce Job definitions, which otherwise
would be implemented in the Java \cite{java}, which would often result in very verbose and hard to understand code (especialy due to the verbosity of Hadoop's core APIs).

Scalding is a thin layer on top

Cascading is a framework built on top Apache Hadoop and enables map reduce authors to think in terms of high level abstractions, such as data ,,flows'' 
and job ,,pipelines'' (series of Map Reduce jobs executed in parallel or sequentially) which have been used extensively in this project.

During the work on this thesis several I have pushed several contributions to the Scalding open source project.
\todo{link contributions}

\todo{link to patches included}
\todo{explain what scalding and cascading are}

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache HBase}
\label{sec:hbase}

HBase is a column-oriented database \cite{columnar-database} designed by following the Google white paper on their ,,BigTable'' datastore published in 2007.
Column oriented storage of data, as opposed to row oriented (as most SQL databases), as huge advantages when many aggregations over only given columns are performed.

It was selected for this project because it's excellent random-access to data as well as being perfectly suited for sourcing Map Reduce tasks.
HBase stores it Tables on the Hadoop Distributed File System, thus it scales similarly to it, which will be proven in a later section in Chapter \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scala}
\label{sec:scala}
Scala is a functional \textit{and} object-oriented programming language designed by Martin Odersky \cite{scala} running on the Java Virtual Machine.
I selected it as primary implementation language for this project (though other languages used include: ANSI C, Ruby and Bash) because of the compelling 
libraries for building distributed systems using it, such as \textit{Akka} and \textit{Scalding} (introduced in the sections \ref{sec:akka} and \ref{sec:scalding}).

It's functional nature (making it similar to languages such as Lisp or Haskell) is very helpful when performing transform / aggregate operations over collections of data. It should be also noted that Hadoop itself was inspired by languages such as this, because the canonical names of the functions 
performing data transformation and aggregation in functional languages are: ,,map'' and ,,reduce''.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Akka}
\label{sec:akka}

Akka is a library providing an Actor Model \cite{actor-model} based concurrency for Scala (and Java) applications. 
This model may be familiar to some as it has gained popularity thanks to Erlang \cite{erlang} which implements the same concepts.

For the sake of this thesis, Akka has been used both in local (in-jvm) parallel execution as well as remote (across nodes) message passing mode,
in order to balance the workload generated by actors across the entire cluster. This is explained in detail in Chapter \todo{where?}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{phash}
\label{sec:phash}
PHash is short for ,,Perceptual Hash'' and is a sort of hashing algorithm (primarily aimed for use with images), which retains enough information to be 
comparable with another has, yielding ,,how similar'' these hashes are. The details and implementation of it have been explained by Christoph Zauner's \cite{phash}.

This algorithm is used by the system to perform initial similarity analysis between images. 
The algorithm is publicly available, including sources (in C), and may be used in non-commercial applications.

As the goal of this thesis is not introducing such algorithm, but focusing on image analysis in distributed systems,
I decided to use the provided implementation and focus on clustering and scaling problems.
\todo{is this needed?}

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Chef}
\label{sec:chef}
Because of how tedious setting up Hadoop clusters is I have early on during the implementation phase of the project decided that cluster 
provisioning and configuration must be fully automated. Opscode's Chef is a tool which enables preparing provisioning scripts in a very readable
way and apply them to a given set of machines.

Using it as well as cloud providers such as Amazon's EC2, Google's ComputeEngine I was able to fully automate a cluster's deployment.

\todo{can i notice ebay?}

Details about the implementation of these recipes are featured in Appendix A.


% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Other tools and technologies used}
\label{sec:other tools}

\subsection{youtube-dl}
Youtube-dl is a small library written in python and freely available under an Open Source license. 
It was used in order to make downloading source video files from youtube more efficient, as it is aware of multiple available video formats (high / low quality), and offers multiple options useful yet hard to implement for this project -- including for example ,,preferring to download open source video formats'', which allowed me to avoid installing proprietary video codecs on the servers.

\subsection{tesseract-ocr}

Tesseract \cite{tesseract} is a text recognition library developed by Google and freely available to use (including language stems for most popular languages).
This tool has been used in order to extract text from analysed images, providing even more data.

\todo{more?}