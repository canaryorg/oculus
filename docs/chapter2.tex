\chapter{Analysis of available technologies}
\label{cha:existing-and-selected-tech}

The primary focus of this work is analysing and benchmarking usage of popular distributed system stacks. 
This section introduces the various technologies used throughout the work on this thesis.

This chapter should be treated as a brief introduction into the selected technologies, as in-depth explanations and and implementation details will be provided throughout chapters \ref{chap:system-design} through \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache Hadoop}
\label{sec:hadoop}

Apache Hadoop is a suite of tools and libraries modelled after a number of Google's most famous whitepapers concerning ''Big Data'', such as \textit{Chubby} \cite{chubby} (on which the \textit{Google Distributed File System} \cite{gfs} was built) and the later, ground breaking, \textit{Map Reduce} \cite{map-reduce} whitepaper concerning parallelisation of computation over massive amounts of data.
The re-implementation of these whitepapers which has become known as Hadoop was originally an implementation used by Yahoo \cite{yahoo-hadoop} internally, and then released to the general public in late 2007 under the Apache Free Software License.

The general use-case of Hadoop based systems revolves around massively parallel computation over humongous amounts of data. Thanks to employing functional programming paradigms in multi-server environments Hadoop makes it possible, and simple, to distribute so called ''Map Reduce Jobs'' across thousands of servers which execute the given \textit{map} (also known as ''\textit{transform}'' or ''emit'') and \textit{reduce} (also known as ''\textit{fold}'') functions in a parallel, distributed fashion. Complex computations, which can not be represented as single Map Reduce job, are often executed as a series of jobs, so called Job Pipelines. This method will be leveraged and explained in detail in Chapter \ref{chap:implementation}, together with the introduction of Scalding (see Section \ref{sec:scalding-info}) -- a Domain Specific Language built specifically to ease building such pipelines.

The promise of Hadoop is practically linear scalability of Hadoop clusters when adding more resources to them. These claims will be investigates in Chapter \ref{chap:perf-scalability}, where results of different cluster configurations will be compared. It's computation model proposed is examined and explained in detail in later sections of this paper, as it is the main model chosen for the implementation of the presented system.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scalding \& Cascading}
\label{sec:scalding-info}

Scalding \cite{scalding} is a Domain Specific Language implemented using the Scala \cite{scala} programming language. It has been developed at Twitter \cite{twitter} for their internal needs, and then released under the GPL license. It is aimed at proving a more expressive and powerful language for writing Map Reduce Job definitions, which otherwise
would be implemented in the Java \cite{java}, which would often result in very verbose and hard to understand code (especially due to the verbosity of Hadoop's core APIs).

Scalding does not stand on it's own, as it is only a thin layer built on top of Concurrent Inc.'s \cite{concurrent-inc} Cascading library, which is a framework built on top of Apache Hadoop. It provides Map Reduce authors to think in terms of high level abstractions, such as data ''flows'' and job ''pipelines'' (series of Map Reduce jobs executed in parallel or sequentially) which have been used extensively in this project.



% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Apache HBase}
\label{sec:hbase}

HBase is a column-oriented database \cite{columnar-database} designed in accordance to the Google ''BigTable'' whitepaper \cite{big-table}, describing their datastore implementation published in 2007.

Column-oriented storage of data, as opposed to row-oriented (as most SQL databases), has huge advantages when many aggregations are performed over only a subset of the columns -- as loading the other columns into memory can be avoided. Because of this specific feature of HBase it is very common o have rows that span thousands of columns, while still remaining efficient.

It was selected for this project because it's excellent random-access to data as well as for being perfectly suited for sourcing Map Reduce tasks. HBase stores it Tables on the Hadoop Distributed File System, thus it scales similarly to it -- because of it's Tables are laid out as Sequence Files that are a very performant way to store data, such as rows of a big table, on HDFS. An in-depth investigation in Hadoop as well as Sequence File performance will be provided in a later section in Chapter \ref{chap:perf-scalability}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Scala}
\label{sec:scala}
Scala is a functional \textit{and} object-oriented programming language designed by Martin Odersky \cite{scala} running on the Java Virtual Machine. It was selected as primary implementation language for this project due to Akka and Scalding libraries. Both libraries are introduced in this section, both allow building distributed systems. Other languages used during this thesis include: ANSI C, Ruby and Bash, yet the vast majority was implemented using Scala.

Scala's functional nature (making it similar to languages such as Lisp or Haskell) is very helpful when performing transform / aggregate operations over collections of data. It should be also noted that Hadoop \textit{itself} was inspired by languages such as this, because the canonical names of the functions performing data transformation and aggregation in functional languages are: ''map'' and ''reduce''.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Akka}
\label{sec:akka}

Akka \cite{akka-docs} is a library providing actor model \cite{actor-model} based concurrency utilities for Scala and Java applications. 
Using this concurrency model can be best explained using two rules ''share nothing'' and ''communicate via messages'' -- both stemming from the programming language Erlang \cite{erlang}. Indeed both Erlang and Akka provide the same concepts and levels of abstraction. The general gain from using this model is that one is oblivious to \textit{where} an Actor  actually is running -- in terms of ''in local JVM'' or ''remotely, and the message will be delivered via TCP/UDP''. Thanks to this not-knowing, it is trivial to scale such applications horizontally, since the code does not need to change when moving from 1-node implementations to clustered environments.

During the work on this thesis, Akka has been used both in local (in-jvm) parallel execution as well as clustered deployment (using Akka's built in clustering module), in order to balance the workload generated by actors across the entire cluster. Details on scaling Akka clusters have been provided in Section \ref{sec:scaling-akka}.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{PHash}
\label{sec:phash}
PHash is short for \textit{Perceptual Hash} and is a sort of hashing algorithm (primarily aimed for use with images), which retains enough information to be comparable with another hash, yielding ,,how similar'' these hashes are. The details and implementation of it have been explained by Christoph Zauner \cite{phash}.

This algorithm is used by the system to perform initial similarity analysis between images. It is publicly available, including it's C source code, and may be used in \textit{non-commercial applications} in accordance to it's license. During the work on this thesis the source code of phash was slightly modified (in agreement with the software's license) to be adjusted to work better by being called from Java applications.

As the goal of this thesis is not researching such algorithms, but focusing on scalable image analysis computations in distributed systems, it was deemed appropriate to use an existing perceptual hashing solution.

% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Chef}
\label{sec:chef}
OpsCode Chef \cite{chef} is a set of tools aimed at automating server configuration and provisioning. Using it enabled automating spinning up new servers which was a very large part of the work on this thesis. Once such automation was prepared, it could     be applied to different cloud service provides -- which led to exploring local virtual machines\footnote{Vagrant, which utilises VirtualBox} as well as Amazon's Elastic Compute Cloud (widely known as ''EC2'') public cloud offering until lastly settling on using Google's Compute Engine public cloud offering. Chef prevented spending many  tedious hour of reinstalling the cluster each time anew. Chef allowed to ''cook'' given virtual machines into the required state -- all software that is required to run the application was installed by one-written chef scripts instead.

Appendix \ref{app:chef} features an in-depth guide on the details on how Chef was used in this thesis, and what steps had to be taken in order to prepare the ''recipes'' it works on to be able to provision Hadoop automatically to any given GNU/Linux running instance.


% ------------------------------------------------------------------------------------------------------------------------------------------------
\section{Youtube-dl}
Youtube-dl \cite{youtube-dl} is a small library written in Python and freely available under an Open Source license. 
It was used in order to make downloading source video files from YouTube more efficient. It is aware of multiple available video formats (high / low quality). It offers multiple options useful yet hard to implement for this project.

Another reason for choosing youtube-dl lies in useful yet hard to implement options it offers. Prime example of such option would be possibility to "prefer downloading open source video formats. Thanks to this option the youtube crawler system was able to download only free formats, for which conversion utilities are freely available on GNU platforms.

%\subsection{tesseract-ocr}
%
%Tesseract \cite{tesseract} is a text recognition library developed by Google and freely available to use (including language stems for most popular languages).
%This tool has been used in order to extract text from analysed images, providing even more data.
%
% remove sesseract IMO...











